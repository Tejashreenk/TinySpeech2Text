{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: This code was pulled directly from Edge Impulse and likely wont have access to the data used to train it. This is for submission use only"
      ],
      "metadata": {
        "id": "MyJDZ46ynFm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxaQWBbzg7FM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, MaxPooling2D, AveragePooling2D, BatchNormalization, Permute, ReLU, Softmax\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "# Data augmentation for spectrograms, which can be configured in visual mode.\n",
        "# To learn what these arguments mean, see the SpecAugment paper:\n",
        "# https://arxiv.org/abs/1904.08779\n",
        "sa = SpecAugment(spectrogram_shape=[int(input_length / 14), 14], mF_num_freq_masks=0, F_freq_mask_max_consecutive=0, mT_num_time_masks=1, T_time_mask_max_consecutive=1, enable_time_warp=False, W_time_warp_max_distance=6, mask_with_mean=False)\n",
        "train_dataset = train_dataset.map(sa.mapper(), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "EPOCHS = args.epochs or 75\n",
        "LEARNING_RATE = args.learning_rate or 0.005\n",
        "# If True, non-deterministic functions (e.g. shuffling batches) are not used.\n",
        "# This is False by default.\n",
        "ENSURE_DETERMINISM = args.ensure_determinism\n",
        "# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\n",
        "BATCH_SIZE = args.batch_size or 32\n",
        "if not ENSURE_DETERMINISM:\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=BATCH_SIZE*4)\n",
        "train_dataset=train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "# model architecture\n",
        "model = Sequential()\n",
        "# Data augmentation, which can be configured in visual mode\n",
        "model.add(tf.keras.layers.GaussianNoise(stddev=0.45))\n",
        "model.add(Reshape((int(input_length / 14), 14), input_shape=(input_length, )))\n",
        "model.add(Conv1D(16, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(classes, name='y_pred', activation='softmax'))\n",
        "\n",
        "# this controls the learning rate\n",
        "opt = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
        "callbacks.append(BatchLoggerCallback(BATCH_SIZE, train_sample_count, epochs=EPOCHS, ensure_determinism=ENSURE_DETERMINISM))\n",
        "\n",
        "# train the neural network\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=EPOCHS, validation_data=validation_dataset, verbose=2, callbacks=callbacks)\n",
        "\n",
        "# Use this flag to disable per-channel quantization for a model.\n",
        "# This can reduce RAM usage for convolutional models, but may have\n",
        "# an impact on accuracy.\n",
        "disable_per_channel_quantization = False"
      ]
    }
  ]
}